%----------------------------------------------------------------------------------------
%	PART III: Abstracts
%----------------------------------------------------------------------------------------

\part{Abstracts}

\chapter*{Abstracts of Minysimposia Talks}

\begin{multicols}{2}

    \noindent\textbf{MS Part 2}\\
\\  
    \textit{To better retrieve task related discriminative source patches, we propose a novel EEG source imaging model based on spatial and temporal graph structures. In particular, graph fractional-order total variation (gFOTV) is used to enhance spatial smoothness, and the label information of brain state is enclosed in a temporal graph regularization term to guarantee intra-class consistency of estimated sources. Numerical experiments have shown that our method localizes source extents more effectively than the benchmark methods.}\\
\\ 
    \myaut{Jing Qin}\\
    \mail{jing.qin@montana.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this talk, we propose an adaptation of  PDE-based level set method for nonmonotonic front propagation on weighted graphs.
A new formulation of the level set equation on weighted graphs considering both time-dependent and stationary versions of this equation in the case of signed velocities is proposed. This formulation leads to an efficient algorithm that generalized the fast marching to graphs with signed velocities. We propose to use this method for image processing and for high-dimensional data classification.}\\
\\ 
    \myaut{Abderrahim Elmoataz}\\
    \mail{abderrahim.elmoataz-billah@unicaen.fr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In deep learning, general guidance to network architecture design remains a mystery. In our work, we bridge deep neural network design with numerical (stochastic) differential equations. We establish a connection between deep architectures and numerical differential equations, which brings us a brand new perspective on the design of effective deep architectures. This enables us to take advantage of the rich knowledge in numerical analysis to guide us in designing new and more effective deep networks. }\\
\\ 
    \myaut{Bin Dong}\\
    \mail{dongbin@math.pku.edu.cn}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk, we present a framework for processing discrete manifold-valued data, for which the underlying (sampling) topology is modeled by a graph. We introduce the notion of a manifold-valued derivative on a graph and based on this exemplarily deduce a family of manifold-valued graph p-Laplace operators. We discuss a simple numerical scheme to compute a solution to corresponding parabolic PDEs and apply this algorithm to different manifold-valued data in denoising and inpainting applications.}\\
\\ 
    \myaut{Daniel  Tenbrinck}\\
    \mail{daniel.tenbrinck@uni-muenster.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We describe a convex relaxation for a certain set of graph-based multiclass data segmentation problems, featuring region homogeneity terms, supervised information and/or certain constraints or penalty terms acting on the class sizes. Particular applications include semi-supervised segmentation/classification of high-dimensional data and unsupervised segmentation of unstructured 3D point clouds. Theoretical and experimental analysis indicates that the convex relaxation closely approximates the original NP-hard problems. An efficient duality based algorithm is also developed.}\\
\\ 
    \myaut{Ekaterina  Rapinchuk}\\
    \mail{kmerkurev@math.msu.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Interpolation on high dimensional point cloud provides a fundamental model in many data analysis and machine learning problems. In this talk, we will present some PDE based methods to do interpolation on point cloud. Applications in image processing and data analysis are shown to demonstrate the performance of our methods.}\\
\\ 
    \myaut{Zuoqiang Shi}\\
    \mail{zqshi@math.tsinghua.edu.cn}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Hyperspectral imaging has emerged as a promising tool in the identification of objects by their chemical and material composition. We discuss a novel graph based method for image segmentation based on the Ginzburg-Landau functional from classical PDE. It aims to preserve as much spectral and structural information as possible. Our experiments are based on real data from a wide field of view imaging spectrometer (WFIS) designed for atmospheric chemistry and aerosols measurements.}\\
\\ 
    \myaut{Yifei Lou}\\
    \mail{yifei.lou@utdallas.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Problems regularized by the total variation on a general weighted graph produce piecewise constant solutions with a small number of constant connected components. We propose a working-set strategy which exploits this structure by recursively splitting the connected components of a candidate solution using graph cuts. This method exhibits a significant speed-up over state-of-the-art algorithms when regularizing ill-posed, ill-conditioned, large-scale inverse problems, notably involving functions with a separable non-smooth part.}\\
\\ 
    \myaut{Loic Landrieu}\\
    \mail{loic.landrieu@ign.fr}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We present a novel quantization algorithm based on building and adaptively pruning a partition on the data space. The proposed method has connections with decision trees, wavelet representation and can be contrasted to classic quantization schemes such as k-means.  The performance of the algorithms are analyzed in a general statistical learning framework where data are assumed to be sample according to un unknown distribution. In particular, the obtained error estimates depend on the geometric properties of the support of the distribution and cover the special case where the latter is a manifold. Joint work with Enrico Cecini  (Universita’ di Genova) and Ernesto De Vito  (Universita’ di Genova)}\\
\\ 
    \myaut{Lorenzo Rosasco}\\
    \mail{lrosasco@mit.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{High density implants such as metals often lead to serious artifacts in the reconstructed CT images which hampers the accuracy of image based diagnosis and treatment planning. In our recent work, we proposed a novel wavelet frame based CT image reconstruction model to reduce metal artifacts. This model is built on a joint spatial and Radon (projection) domain (JSR) image reconstruction framework with a built-in weighting and re-weighting mechanism in Radon domain to repair degraded projection data. The new weighting strategy used in the proposed model not only makes the regularization in Radon domain by wavelet frame transform more effective, but also makes the commonly assumed linear model for CT imaging a more accurate approximation of the nonlinear physical problem. The proposed model, which will be referred to as the re-weighted JSR model, combines the ideas of the recently proposed wavelet frame based JSR model (Dong, Li and Shen, 2013) and the normalized metal artifact reduction model (Meyer et al. 2010), and manages to achieve noticeably better CT reconstruction quality than both methods. To solve the proposed re-weighted JSR model, an efficient alternative iteration algorithm is proposed with guaranteed convergence. Numerical experiments on both simulated and real CT image data demonstrate the effectiveness of the re-weighted JSR model and its advantage over some of the state-of-the-art methods.}\\
\\ 
    \myaut{Bin Dong}\\
    \mail{dongbin@math.pku.edu.cn}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We consider reconstructing a spectrally sparse signal with super-resolution from its partial revealed entries. By utilizing the low-rank structure of the Hankel matrix, we develop a computationally efficient algorithm for this problem. The algorithm is a projected gradient descent for a non-convex functional. We prove that O(r2log(n)) observed entries are sufficient for our algorithm to achieve the successful recovery of a spectrally sparse signal. Our algorithm is competitive with other state-of-the-art spectral compressed sensing algorithms.}\\
\\ 
    \myaut{Jian-Feng  Cai}\\
    \mail{jfcai@ust.hk}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{This talk is concerned with the super-resolution problem for positive spikes in arbitrary dimensions. More precisely, I will discuss the issue of support recovery for the so-called BLASSO method. While super-resolution is of paramount importance in overcoming the limitations of many imaging devices, its theoretical analysis is still lacking beyond the 1-dimensional case. The reason is that in the 2-dimensional case and beyond, the relative positions of the spikes enter the picture, and one needs to account for these different geometrical configurations. After presenting an algorithmic description of the limit of the associated dual problems as the spikes cluster around a given point, I will present a detailed analysis of the support stability and super-resolution effect in the case of a pair of spikes.}\\
\\ 
    \myaut{Clarice Poon}\\
    \mail{c.m.h.s.poon@maths.cam.ac.uk}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Recent technical advances lead to the coupling of PET and MRI scanners, enabling to acquire functional and anatomical data simultaneously. In this talk, we propose a tight frame based PET-MRI joint reconstruction model via the joint sparsity of tight frame coefficients. In addition, a non-convex balanced approach is adopted to take the different regularities of PET and MRI images into account. To solve the nonconvex and nonsmooth model, a proximal alternating minimization algorithm is proposed, and the global convergence is present based on Kurdyka-Lojasiewicz property. Finally, the numerical experiments show that the our proposed models achieve better performance over the existing PET-MRI joint reconstruction models.}\\
\\ 
    \myaut{Jae Kyu Choi}\\
    \mail{jaycjk@sjtu.edu.cn}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Given massive amounts of data, is it possible to learn a sparsifying basis? This dictionary learning problem has witnesses intensive algorithmic developments and empirical successes over the past two decades. In contrast, only recently has theoretical understanding on dictionary learning surfaced. In this talk, I will summarize the recent theoretical progress, and highlight the surprising role that nonconvex optimization plays in provable and practical dictionary learning, and beyond. }\\
\\ 
    \myaut{Ju Sun}\\
    \mail{sunju@stanford.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Kernel-based Maximum Mean Discrepancy (MMD) statistics have been widely used to measure the distance between distributions from finitely-many samples. This talk introduces an MMD statistic involving anisotropic kernels which capture the locally low-dimensional structures of the underlying distributions. The proposed test is theoretically shown to be more powerful to distinguish certain alternatives than its isotropic-kernel counterpart. Compared to the usual kernel MMD tests, the method has lower computational cost due to using a drastically downsampled reference set. Applications to flow cytometry data and diffusion MRI image datasets will be discussed.}\\
\\ 
    \myaut{Xiuyuan Cheng}\\
    \mail{xiuyuan.cheng@duke.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{A now standard method for recovering the unknown signal is to solve a convex optimization problem that enforces some prior knowledge about its structure. I will deliver a review of recent advances in the field where the regularization prior promotes solutions conforming to some notion of simplicity/low-complexity. Our aim is to provide a unified treatment of all these regularizations under a single umbrella, namely the theory of partial smoothness.}\\
\\ 
    \myaut{Samuel  Vaiter}\\
    \mail{samuel.vaiter@u-bourgogne.fr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We combine confocal microscopy and imaging spectroscopy to determine spatial morphology and chemical composition of a target in three spatial dimensions from backscattered light. We assume the target comprises few chemical species with known spectra and develop conditions on the spectra and number of measurements for unique image recovery. Images are formed by solving a regularized least squares problem using an iterative algorithm. Simulations illustrate imaging of cellular phantoms and sub-wavelength  targets from noisy measurements.}\\
\\ 
    \myaut{Yoram Bresler}\\
    \mail{ybresler@illinois.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Convolutional neural networks are revolutionizing imaging science for two and three dimensional images over Euclidean domains. However, many images, and more generally data sets, are intrinsically non-Euclidean and are better modeled through other mathematical structures, such as graphs or manifolds. This has led in recent years to the development of geometric deep learning, which refers to a body of research that aims to translate the principles of convolutional neural networks to these non-Euclidean structures. In the process, various challenges have arisen, including how to define such networks, how to compute and train them efficiently, and what are their mathematical properties. In this talk, we will focus on the first and last questions, ignoring for now issues of computation. We define a class of geometric networks defined over manifolds, and illustrate that such networks naturally encode localized isometry invariant descriptions of data, which generalizes local translation and rotation invariance on Euclidean domains. We additionally show conditions under which such networks will be provably stable feature extractors, which requires linking the structure of the individual filters to the geometry of the manifold. Joint work with Michael Perlmutter (Michigan State University) and Guy Wolf (Yale University).}\\
\\ 
    \myaut{Matthew Hirn}\\
    \mail{mhirn@msu.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We consider the problem of filtering a set of images lying on a low dimensional manifold, under the assumption that the in-plane rotation of each image is irrelevant. We derive the steerable graph Laplacian on the image-manifold, which accounts for all planar rotations of all images, and show how to use it for image filtering while exploiting all images and their rotations simultaneously. We demonstrate our approach for the denoising of cryo-electron microscopy image datasets.}\\
\\ 
    \myaut{Boris Landa}\\
    \mail{borisla2@post.tau.ac.il}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{I will talk about a new approach to linear ill-posed inverse problems with data-driven regularization. Instead of learning a stable inverse, unrolling standard algorithms into neural nets, or learning projectors for iterative schemes, we still compute the solution as a minimizer of a regularized cost functional, albeit non-convex. Our regularizer promotes "correct" conditional statistics in some feature space. As feature transform we choose the non-linear multiscale scattering transform---a complex convolutional network which discards the phase and thus exposes spectral correlations otherwise hidden beneath the phase fluctuations. We need scale separation in order to guarantee stability to deformations. For a given realization, the feature-space representation is linearly estimated from a reconstruction in a stable subspace and it represents the unstable part of the signal. We demonstrate that our approach stably recovers the missing spectrum in super-resolution and tomography. }\\
\\ 
    \myaut{Ivan Dokmanic}\\
    \mail{dokmanic@illinois.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Advances in deformable template matching has allowed us to investigate various different biological phenomena, from computational anatomy to kinematics.  However, there is still more to be explored in what is capable by these models.  In particular, the theory of metamorphosis, proposed by Younes, Trouve, and Miller, can be used to assess the similarity between two shapes by solving a variational problem that mixes the deforming of the domain and the modulating of the range of the shape template. This captures more modes of variation than the other pure deformation models, and can be used as a metric to define a Riemannian space of shapes, allowing for additional insights about shape similiarity.
\
In this talk, I will discuss the foundations of metamorphosis in the one dimensional case, as well as provide two additional insights.  The first is theory that allows us to solve for one of the variables in the alternating minimization in closed form.  The second is a construction of Schild's ladder in the metamorphosis metric space, allowing us to perform parallel transport and adequately compare tangent vectors in the shape space.  Additionally, I will show that by using these insights on the shape space of heart cell action potentials, one can suggest the phenotype (atrial vs ventricular) of a newly differentiated heart muscle cell, as
well as determine whether a heart cell has a modification that makes it more/less susceptible to a given drug treatment.}\\
\\ 
    \myaut{Rene Vidal}\\
    \mail{rvidal@jhu.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We discuss strategies and recent results for lifting- and measure-valued approaches to variational problems with values in a manifold. The approaches can be naturally used to regularize problems where the unknowns are orientation distribution functions, in particular in diffusion-weighted imaging.}\\
\\ 
    \myaut{Jan Lellmann}\\
    \mail{lellmann@mic.uni-luebeck.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Multi-Angle Total Internal Reflection Fluorescence (MA-TIRF) microscopy is a recent technique which provides depth information. Its axial resolution is typically 20 nm, but the lateral resolution is unfortunately not as good.
We investigate the combination of MA-TIRF and single molecule emission microscopy techniques, so as to remove that shortcoming. This yields an inverse problem where the measurements combine partial Laplace and convolution operations. We present a gridless total-variation minimization approach, with theoretical and numerical results. }\\
\\ 
    \myaut{Vincent Duval}\\
    \mail{vincent.duval@inria.fr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We study the discretization of maps from a Euclidean domain into a smooth Riemannian manifold minimizing an elliptic energy. The discretization is given by a finite-dimensional approximation of the set of functions, such that the target manifold is neither embedded nor approximated. In particular, we discuss two constructions, namely geodesic and projection-based finite elements. Both have the properties needed for an error analysis comparable to standard Euclidean finite elements.}\\
\\ 
    \myaut{Hanne Hardering}\\
    \mail{hanne.hardering@tu-dresden.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{A novel approach  is introduced to Maximum A Posteriori inference based on discrete graphical models. 
The given discrete objective function is smoothly approximated using regularized local Wasserstein distances in order
 to couple assignment measures across edges of the underlying graph. This approximation is restricted to the
 assignment manifold and optimized by a multiplicative update combining (i) geometric integration of the resulting Riemannian gradient flow and (ii) rounding to integral solutions that represent valid labelings.}\\
\\ 
    \myaut{Ruben Hühnerbein}\\
    \mail{ruben.huehnerbein@iwr.uni-heidelberg.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The problem of averaging symmetric positive-definite (SPD) matrices arises for example in medical imaging (denoising and segmentation tasks in Diffusion Tensor Imaging), mechanics (elasticity tensor computation), and in video tracking and radar detection tasks. We will review recent advances in iterative methods that converge to the SPD geometric mean (namely the least-squares mean in the sense of the so-called affine-invariant metric), and in methods that approach it using limited resources.}\\
\\ 
    \myaut{Pierre-Antoine Absil }\\
    \mail{absil@inma.ucl.ac.be}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{The total generalized variation (TGV) functional provides a convex model for piecewise smooth vector-space data and is amongst the most successful regularization functionals for variational image reconstruction. In this talk, we introduce the notion of second-order TGV regularization for manifold-valued data. We provide an axiomatic approach to formalize reasonable generalizations of TGV to this setting and present concrete instances that fulfill the proposed axioms. We prove well-posedness results and present numerical algorithms and experimental results.}\\
\\ 
    \myaut{Martin Holler}\\
    \mail{martin.holler@uni-graz.at}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{When dealing with manifold-valued data one faces the same challenging
processing tasks as, e.g., in classical imaging.
In this talk we consider image inpainting for manifold-valued data in which
missing information have to be filled in suitably. We present a generalization
of the graph infinity-Laplacian to manifold-valued data based on the
min-max characterization of the local discrete Lipschitz constant. We derive a
numerical scheme to solve the obtained manifold-valued infinity-Laplace equation
and inpaint missing data.}\\
\\ 
    \myaut{Ronny Bergmann}\\
    \mail{bergmann@mathematik.uni-kl.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Euler's elastica energy (the mean squared acceleration) of parameterized curves may serve as a regularization for fitting or approximating temporal data. We study the case of data in a Riemannian manifold, as is relevant for various applications such as keyframe interpolation in computer graphics or interpolation in the space of images, equipped with an appropriate Riemannian metric. The analysis of the energy and its discretization hold some surprises fundamentally different from the Euclidean setting.}\\
\\ 
    \myaut{Benedikt Wirth}\\
    \mail{benedikt.wirth@uni-muenster.de}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{In the metamorphosis approach the space of images is considered as a Riemannian manifold.
In this talk, we focus on the computation of time discrete geodesics
defined as minimizers of time discrete path energies.
Here, images are either considered as square-integrable intensity functions
or regarded as a superposition of sparse signals convoluted
with structure classifying learned kernels.
In the first case, the Gamma-convergence of the time discrete model to the time continuous model is discussed. }\\
\\ 
    \myaut{Alexander Effland}\\
    \mail{alexander.effland@ins.uni-bonn.de}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We develop a general mathematical framework for variational problems where the unknown function assumes values in the space of probability measures on some metric space. We study suitable weak and strong topologies and define a total variation seminorm for functions taking values in a Banach space. For a class of variational problems based on this formulation, we prove existence and point out connections to the Kantorovich-Rubinstein norm and optimal transport.}\\
\\ 
    \myaut{Thomas Vogt}\\
    \mail{thomas.vogt@mic.uni-luebeck.de}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Curvature regularization of image level lines is a powerful tool in image processing. Using so-called functional lifting, this can be achieved by specific convex functionals in a higher-dimensional space. The functional requires a subtle discretization of a Radon measure to fulfill a compatibility condition and to give reasonable results. Additionally, the resulting high computational costs have to be managed. We present an adaptive discretization and give some results for image segmentation for 2D- and 3D-images.}\\
\\ 
    \myaut{Ulrich Hartleif}\\
    \mail{u\_hart02@uni-muenster.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In magnetic particle imaging potential measurements are obtained by exploiting the particle’s nonlinear response to an applied dynamic magnetic field to recover the particle concentration. It is frequently modeled by a linear Fredholm integral equation of the first kind. In this talk we analyze the inverse problem obtained from the equilibrium model which is based on the Langevin function. We further discuss how particle relaxation influences image reconstruction in MPI.}\\
\\ 
    \myaut{Tobias Kluth}\\
    \mail{tkluth@math.uni-bremen.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The forward operator in magnetic particle imaging is given by an integral equation of the first kind. The integration kernel is called the system function and describes the potential of magnetic particles to induce a signal in the receive coils by a change in its magnetization. The system function is not known explicitly and demands for a data-driven computation that must also include relaxation effects. The talk subsumes actual modeling aspects.}\\
\\ 
    \myaut{Anne Wald}\\
    \mail{wald@math.uni-sb.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Subtracting empty scanner measurements reduces the influence of stationary background artifacts, but also requires additional computation and acquisition time while recording system matrices. 
Experimental evidence indicates that background subtractions can be replaced by a suitable high-pass-filtering of complex 2D-DCT coefficients of the system matrix. 
In combination with soft-thresholding our approach also reveals frequency components which partially remain buried in stationary noise even after background correction.  
This might lead to further improvements of image reconstructions.}\\
\\ 
    \myaut{Hans-Georg Stark}\\
    \mail{hans-georg.stark@h-ab.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In the context of MPI approaches measuring the system's response to shifted delta peaks 
together with algebraic reconstruction methods are widely used;
due to reasons of time consumption, also in practice, model-based approaches are desirable.
The talk considers model based reconstruction for MPI with 
a particular emphasis on the multivariate situation. 
We investigate the MPI core operator and its ill-posedness properties.
We obtain reconstruction formulae in 2D and 3D.  }\\
\\ 
    \myaut{Andreas Weinmann}\\
    \mail{andreas.weinmann@h-da.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{The relation between the measured signal and the particle distribution in MPI can be described by a linear system of equations. Since it is challenging to precisely model the particle physics, the system matrix is usually explicitly arranged. In this work we will present an alternative approach that approximates the individual matrix rows by tensor products of Chebyshev polynomials. This allows to fully diagonalize the MPI system matrix and in turn enables fast reconstruction.}\\
\\ 
    \myaut{Knopp Tobias}\\
    \mail{t.knopp@uke.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Magnetic Particle Imaging (MPI) is capable of capturing fast dynamic processes in 3D volumes, based on the non-linear response of the magnetic nanoparticles to an applied magnetic field. The image reconstruction is computationally demanding due to a non-sparse system matrix. In this study we propose a quadratic spatio-temporal regularization that can be efficiently solved. Results are presented for simulated and experimental measurement data.}\\
\\ 
    \myaut{Andreas Hauptmann}\\
    \mail{a.hauptmann@ucl.ac.uk}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this talk, we will discuss the usage of spectral methods for an efficient reconstruction In Magnetic Particle Imaging. In particular, we will analyze how the usage of different Chebyshev transforms leads to a sparse representation of the underlying system matrix and how sampling on particular node points of the Lissajous acquisition paths can be combined with the Chebyshev transform in order to get a fast reconstruction of the particle density.}\\
\\ 
    \myaut{Wolfgang Erb}\\
    \mail{erb@math.hawaii.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We develop an efficient edge preserving and noise reducing reconstruction method for MPI using a nonnegative fused lasso model. We devise a discretization adapted to the acquisition geometry of a preclinical MPI scanner and an efficient solver for that setup. Our prototype implementation processes a 3D volume within a few seconds. We demonstrate the improvement in reconstruction quality over the state-of-the-art method in an experimental medical setup for in-vitro angioplasty.}\\
\\ 
    \myaut{Martin Storath}\\
    \mail{martin.storath@iwr.uni-heidelberg.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We present efficient solvers for diffeomorphic registration problems in the framework of Large Deformations Diffeomorphic Metric Mappings (LDDMM). We use an optimal control formulation in which the (stationary or instationary) velocity field of a hyperbolic PDE needs to be chosen in order to minimize the distance between the final state of the system (the transformed/transported template image) and the observation (the reference image). Our formulation is widely applicable as it allows solving mass- and intensity-preserving registration problems.}\\
\\ 
    \myaut{Lars Ruthotto}\\
    \mail{lruthotto@emory.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Deformable image registration is a key technology for image analysis. Traditionally, approaches for deformable image registration have focused on well-defined mathematical models that allow inferring spatial deformations between image pairs via optimization. However, most recently a number of approaches have been proposed that replace optimization by training appropriate regression models from data. This talk will discuss some of these recent developments.}\\
\\ 
    \myaut{Marc Niethammer}\\
    \mail{mn@cs.unc.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We present a novel framework to construct statistical deformation models for diffeomorphisms, aiming to improve the robustness of deformable registration in the presence of pathologies. We model the high-dimensional velocity field as a collection of local velocity fields. For each local field, we learn a low-dimensional representation using principal component analysis. Dependencies across local transformations are captured through canonical correlation analysis. We showcase the improved robustness of the proposed method using simulated brain lesion images as well as real brain images with pathologies.}\\
\\ 
    \myaut{Aristeidis Sotiras}\\
    \mail{aristeidis.sotiras@uphs.upenn.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We investigate the interpolation of image time series using the metamorphosis model of a manifold of images. Based on a variational time discretization, discrete geodesic paths in this space of images are computed. The space discretization is based on finite elements spanned by tensor product cubic B-splines. An efficient implementation is obtained by utilizing a proper combination of GPU and CPU computation. Numerical results of the approach on optical coherence tomography image series are shown.}\\
\\ 
    \myaut{Benjamin Berkels}\\
    \mail{berkels@aices.rwth-aachen.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk, we propose the use of optimal transport for diffeomorphic registration of embedded surfaces and other sparse data. We also propose an extension to the space of images. This type of global similarity measures between data relies on the use of an embedding of data to a space of measures and the use of the entropic regularization of a generalization of the Wasserstein metric. This method can be generalized to many inverse problems.}\\
\\ 
    \myaut{François-Xavier Vialard}\\
    \mail{fxvialard@normalesup.org}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{One keytask in modern medical imaging is image registration. It is the task of spatially aligning two or more images. A major problem is the similarity of images. To measure similarity, so-called distance measures are applied. We present a novel
measure using Schatten-q-Quasinorms which are based on Singular Value Decompositions of a matrix of the images' gradients.
The theoretical background is discussed and promising results are presented.}\\
\\ 
    \myaut{Kai Brehmer}\\
    \mail{brehmer@mic.uni-luebeck.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{TBD}\\
\\ 
    \myaut{Alain Trouvé}\\
    \mail{trouve@cmla.ens-cachan.fr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We present CLAIRE, a parallel solver for constrained large deformation diffeomorphic image registration in three dimensions. Our contributions are the following: (i) We present an improved implementation of our memory-distributed, globalized, preconditioned Newton--Krylov solver. (ii) We present effective techniques to precondition the reduced space Hessian. (iii) We study numerical accuracy, rate of convergence, time-to-solution, inversion quality, and scalability of our solver.
\
We use a PDE-constrained formulation for diffeomorphic image registration. The PDE constraints are the transport equations for the image intensities. The control variable is the velocity field. The discretization of the optimality system leads to high-dimensional, ill-conditioned, multiphysics systems that are challenging to solve in an efficient way. Our code is implemented in C/C++ and uses the message passing interface (MPI) library for parallelism. We study the performance of our solver for multi-subject registration problems in neuroimaging. We will see that our solver is competitive in terms of time-to-solution and registration quality. We will see that we can solve problems on clinical images (50 million unknowns) in less than two minutes on a workstation with 40 cores. If we use 512 MPI tasks we can reduce the runtime to under 2 seconds, paving the way to tackle real-time applications.}\\
\\ 
    \myaut{Andreas Mang}\\
    \mail{andreas@math.uh.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk we present the Regularization by Denoising (RED) framework: using a denoiser in defining the regularization of any inverse problem. This scheme leads to well-founded iterative algorithms in which the denoiser is applied in each iteration. We describe how RED defines a novel convolutional neural network architecture, where the point-wise nonlinearities are replaced by denoising. Interestingly, the training of this network leads to faster and better recovery than its analytic origin.}\\
\\ 
    \myaut{Peyman Milanfar}\\
    \mail{peyman.milanfar@gmail.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Regularization-by-denoising (RED) is an image recovery framework recently proposed by Romano, Elad, and Milanfar that allows arbitrary denoiser subroutines to be used with arbitrary convex optimization algorithms to solve a wide range of image recovery problems. We provide new interpretations and new algorithmic solutions to RED, and demonstrate our methods on cardiac imaging via parallel MRI.}\\
\\ 
    \myaut{Phil Schniter}\\
    \mail{schniter@ece.osu.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{I will discuss our recent work on patch-based models that are adapted to specific image classes, or even specific scenes. These models take the form of minimum mean squared error (MMSE) patch-based denoisers, using Gaussian mixture priors. We illustrate their use beyond image denoising, in more general inverse problems, such as inpainting, deblurring, and hyperspectral super-resolution, using the recently introduced plug-and-play approach.}\\
\\ 
    \myaut{Mário Figueiredo}\\
    \mail{mtf@lx.it.pt}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{The MAP model is a famous framework in the field of image restoration, and recent years have also witnessed the unprecedented success of CNNs in image denoising and super-resolution. In this talk, we intend to introduce some works on CNNs by referring to MAP inference in low level vision tasks: (1) design of a CNN denoiser, (2) extension of CNN denoisers for image restoration, and (3) other insights on CNN-based models delivered by MAP inference. }\\
\\ 
    \myaut{Wangmeng Zuo}\\
    \mail{cswmzuo@gmail.com}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Today's imaging and computer vision and imaging systems fail in critical scenarios, for example in low light or in fog. This is due to ambiguities in the captured images, introduced by imperfect capture systems, or present in the signal itself. In this talk, I will present several applications that resolve ambiguities by replacing traditional pipeline image processing with joint Bayesian estimates relying on physical forward models and denoising priors, enabling robust imaging in extreme scenarios.}\\
\\ 
    \myaut{Felix Heide}\\
    \mail{fheide@stanford.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Finding strong oracle priors is an important topic in image restoration. In this talk, I will show how denoising autoencoders (DAEs) learn to mean-shift in O(1), and how we leverage this to employ DAEs as  generic priors for image restoration. I will also discuss the case of Gaussian DAEs in a Bayesian framework, where the degradation noise and/or blur kernel are unknown. Experimental results demonstrate state of the art performance of the proposed DAE priors.}\\
\\ 
    \myaut{Siavash Arjomand Bigdeli}\\
    \mail{bigdeli@inf.unibe.ch}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{While numerous low-level computer vision problems such as denoising, deconvolution or optical flow estimation were traditionally tackled with optimization approaches such as proximal methods, recently deep learning approaches trained on numerous examples demonstrated impressive and sometimes superior performance on respective tasks.
\
In my presentation, I will discuss recent efforts to bring together these seemingly different paradigms, showing how deep learning
can profit from proximal methods and how proximal methods can profit from deep learning.}\\
\\ 
    \myaut{Daniel Cremers}\\
    \mail{cremers@tum.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Inverse problems appear in many image processing applications and are usually addressed by designing task-specific algorithms. The recently proposed Plug-and-Play (P\&P) framework allows solving general inverse problems by leveraging the impressive capabilities of existing denoising techniques. However, P\&P often requires a burdensome parameter tuning. We propose an alternative method for solving inverse problems using denoising algorithms that require less parameter tuning. We demonstrate its competitiveness with task-specific techniques and P\&P for image inpainting and deblurring.}\\
\\ 
    \myaut{Raja Giryes}\\
    \mail{raja@tauex.tau.ac.il}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this talk, we will investigate a reconstruction problem that arises in limited data photoacoustic tomography with a flat observation surface. In the first part of this talk, we will explain which singularities of the original object can be reliably reconstructed and why artifacts can be generated when applying the classical FBP-type reconstruction operators to limited data. We will also provide precise characterizations of added artifacts and explain how they can be reduced. In the second part of this talk, we will present a stable reconstruction method, which is based on sparsity assumptions in the wavelet  domain. In particular, we will present an easy to implement numerical algorithm for that problem.
\
This is based on a joint work with Eric Todd Quinto (Tufts University) and Markus Haltmeier (University of Innsbruck).}\\
\\ 
    \myaut{Jürgen Frikel}\\
    \mail{juergen.frikel@oth-regensburg.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Compressed sensing is a promising approach for significantly reducing the number of measurements in photoacoustic tomography (or other limited data imaging problems) while preserving its high spatial resolution. In this talk we present a new sparse recovery framework for recovering the photoacoustic source from compressive measurements.  Results with simulated as well as experimental data are given. (Joint work with Linh Nguyen, Michael Sandbichler, Thomas Berer, Johannes Bauer-Marschallinger, Peter Burgholzer)}\\
\\ 
    \myaut{Markus Haltmeier}\\
    \mail{ma.haltmeier@gmail.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Learning-based reconstruction approaches require a suitable network architecture, training data, and a loss function, to measure the similarity between the reconstructed image and a reference image during training. In this talk, we give an overview of our current developments using variational networks for accelerated MR image reconstruction and discuss several challenges that are encountered during learning, focusing on how the design of the loss function influences reconstruction quality.}\\
\\ 
    \myaut{Kerstin Hammernik}\\
    \mail{hammernik@icg.tugraz.at}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Compton imaging is a nascent concept arising from the current development of high-sensitive energy detectors and based on the Compton effect, i.e. the scattering of a photon by an electron.  Such detectors are able to collect incoming photons in terms of energy. It follows applications exploiting the scattering radiation to image the electron density of the studied medium.  This presentation introduces potential 3D modalities in Compton imaging as well as the corresponding limited data issues.}\\
\\ 
    \myaut{Gaël Rigaud}\\
    \mail{rigaud@num.uni-sb.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In recent years, machine learning has proved to be successful in several imaging fields. In this talk, different approaches for applying machine learning to challenging imaging problems involving limited data will be discussed. For example, in low-dose tomography problems, neural networks can be used to improve reconstruction quality, enabling analysis of new types of samples. Results will be shown for various problems, and important practical considerations, e.g. computational requirements, will be discussed.}\\
\\ 
    \myaut{Daniel Pelt}\\
    \mail{daan.pelt@gmail.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We will describe our characterization of artifacts in limited data X-ray tomography reconstructions with arbitrary data.  We provide estimates of the strength of the added artifacts in some cases, and we illustrate our results using standard and non-standard limited data tomography problems with real and simulated data. The work is based on a microlocal analysis of the X-ray transform and backprojection, and it can be applied to a range of  limited data problems. }\\
\\ 
    \myaut{Todd Quinto}\\
    \mail{todd.quinto@tufts.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Our work on iterative image-reconstruction applied to Digital Breast Tomosynthesis is split in two efforts:  identifying optimization-based algorithms that provide useful images with few iterations, and developing image quality metrics that guide the parameter settings of the image-reconstruction algorithms. We prototype many optimization-based approaches, involving total-variation, using the Chambolle-Pock primal-dual algorithm. We also present image quality metrics tailored to the task of tumor/mass classification, and discuss the difference in image properties with respect to scan-angle.}\\
\\ 
    \myaut{Emil Sidky}\\
    \mail{sidky@uchicago.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Examples of tomographic imaging of moving objects: recovering the structure of blood vessels using injected contrast agent, and analysing penetration of fluid into porous media. There is often a need to keep the data collection as short as possible and to reduce the radiation dose. One can do this by taking fewer projection images and shortening exposures, leading to scarce-angle tomography with noisy data. Different approaches are compared: optical flow, shearlet sparsity, and Kalman filtering.}\\
\\ 
    \myaut{Samuli Siltanen}\\
    \mail{samuli.siltanen@helsinki.fi}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{In multi-parameter full-waveform inversion (FWI) the choice of parameterization is fundamental to correctly separate the different parameter classes. We investigate the influence of the parameterization on the parameter separation for an elastic isotropic FWI problem from a mathematical standpoint. We also study this influence numerically using a simple model containing multiple anomalies in each parameter class positioned at different locations.}\\
\\ 
    \myaut{Ettore  Biondi }\\
    \mail{ettore88@stanford.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Multiple seismic data sets are often recorded to monitor changes in Earth properties. Results from studies using Full Waveform Inversion (FWI) to recover 4D changes have been encouraging thus far. Since 4D monitoring involves looking for small changes in localized regions, understanding the uncertainty in the measurement of those changes is key. We present an efficient way of creating big samples of data in a fast and computationally inexpensive way. We then use them in a statistical inversion technique to evaluate the performance of current 4D FWI techniques.}\\
\\ 
    \myaut{Maria Kotsi}\\
    \mail{mk7251@mun.ca}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Surface wave data are processed to retrieve dispersion curves that are inverted to estimate velocity models of the subsurface. Recent approaches avoids the inversion step using data transforms that estimate directly the velocity. Dispersion curves are discontinuous and noisy data that requires interpolation and smoothing.}\\
\\ 
    \myaut{Valentina Socco}\\
    \mail{valentina.socco@polito.it}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this work, we present an alternative approach to re-datum both sources and receivers at depth, under the framework of reflectivity-based extended images with two-way wave propagation in the background medium. In our work, we will consider a linear algebra approach to deal with the low-rank representation of extended image volumes with full offsets. We will never build entirely the resulting matrix but get only actions of it on well-chosen probing vectors, based on Low-Rank decomposition or randomized SVD. The proposed scheme  allows us to have access to all the energy of the extended image volume matrix and still overcome the computational cost and memory usage associated with the number of wave-equation solutions and explicit storage employed by conventional migration methods. Experimental results on complex geological models demonstrate the efficacy of proposed methodology in performing multi-domain target imaging.}\\
\\ 
    \myaut{Rajiv Kumar}\\
    \mail{rajmittal09@gmail.com}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Full-Waveform Inversion attempts to estimate a high-resolution model of the Earth by inverting all the seismic data. This procedure fails if the Earth model contains high-contrast bodies such as salt. These bodies are important for hydrocarbon exploration. We propose a parametric level-set method to estimate these geometries by incorporating prior information about their properties. Tests on a suite of idealized salt geometries show that the proposed method is stable against a modest amount of noise.}\\
\\ 
    \myaut{Ajinkya  Kadu}\\
    \mail{a.a.kadu@uu.nl}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We consider an inverse problem for the acoustic wave equation, where an array of sensors probes an unknown medium with pulses and measures the scattered waves. The goal is to determine from these measurements the structure of the scattering medium, modeled by a spatially varying acoustic impedance function. Many conventional inversion algorithms assume that the dependency of the scattered waves on the unknown impedance is approximately linear. The linearization, known as the Born approximation, is not accurate in strongly scattering media, where the waves undergo multiple reflections before reaching the sensors. This results in artifacts in the impedance reconstructions. We show that it is possible to remove the multiple scattering effects from the data, using a reduced order model (ROM). The ROM is an orthogonal projection of the wave equation propagator on the subspace spanned by the time domain snapshots of the wavefields. While the snapshots are only known at sensor locations, this information is enough to construct the ROM. Once the ROM in constructed, we use its perturbations to generate a new data set that the same impedance would generate if the waves in the medium propagated according to Born approximation. We refer to such procedure as the Data-to-Born transform. Once the multiple scattering effects are removed from the data by the transform, it can be fed to conventional linearized inversion workflows.}\\
\\ 
    \myaut{Alexander Mamonov }\\
    \mail{mamonov@math.uh.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Retrieving detailed and accurate images of targets that lie beneath or behind unknown complex overburdens or obstacles is a highly challenging problem in waveform-based imaging, such as in seismic, acoustic or radar applications. This problem is particularly difficult when experimental limitations are such that the medium in question cannot be fully surrounded by both by sources and receivers, thus only limited aperture, one-sided  scattered-wave data are available. Overcoming some of the issues arising from having one-sided data, we will present an imaging framework based on wavefield redatuming, i.e., on retrieving scattered fields within the medium where observations are otherwise not available, that decouples the influence of the overburden from that of the target in imaging and inversion: thus separately allowing for better target images and/or overburden/obstacle characterization.  The key enabler for this is solving an intermediate inverse scattering problem for the medium’s focusing functions, in the context of 3D Marchenko field equations: these fields encode the effects of different portions of the medium without the need to first characterize medium properties. In this talk, we will review the 3D Marchenko system, discuss the theoretical and numerical inverse-problem aspects of retrieving focusing functions, and show examples of imaging options from one-sided data that are enabled by this framework. }\\
\\ 
    \myaut{Ivan Vasconcelos}\\
    \mail{i.vasconcelos@uu.nl}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The Rosetta mission to comet 67P/C-G revolutionized comet science, but left major questions on the table. Are cometary nuclei primordial or are they collisionally-evolved as predicted by modern theories of planet formation? The CONSERT radar experiment sounded the interior of the nucleus showing it to be transparent to a depth of kilometers at 90 MHz, thus demonstrating the feasibility of a 3D global reflection tomography.
\
The Comet Radar Explorer mission will acquire a dense network of in-phase radar echoes from orbit to obtain a high resolution 3D image. Full wavefield tomography facilitate high quality imaging of the comet interiors, particularly if the comet nucleus is characterized by complex structure and large contrasts of physical properties. Knowledge of the comet shape and all-around orbital radar acquisition enable accurate and computationally-efficient 3D wavefield tomography.}\\
\\ 
    \myaut{Paul Sava}\\
    \mail{psava@mines.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The full waveform inversion (FWI) is widely used to obtain images using recorded waveforms and it can be cast into a global nonlinear optimization problem. There are many known challenges in FWI. Using time-space causality of the wavefield, we propose to convert the global nonlinear optimization into many local linear inversions that can be directly solved (DWI). The conversion has no information loss. DWI naturally uses all data types and is unconditionally convergent and efficient.}\\
\\ 
    \myaut{Yingcai Zheng}\\
    \mail{yzheng12@uh.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Numerous applications in seismic image analysis require matching two or more images. Examples include time-lapse and multicomponent image registration, migration deconvolution, full waveform inversion, adaptive subtraction of multiples, etc. Some applications benefit from separating the matching procedure into components, such as scaling, shifting, and smoothing. I review different techniques for seismic image matching and compare them using synthetic and field data examples.}\\
\\ 
    \myaut{Sergey  Fomel }\\
    \mail{sergey.fomel@beg.utexas.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{This talk considers the basic question of frequency extrapolation of bandlimited recordings of scattered waves. I will discuss two methods that were shown to give meaningful results for seismic imaging: (i) a model reduction approach, where the phases of atomic seismic events are estimated by tracking, and (ii) a model extension approach, based on TV-regularized least-squares inversion of the extended Born modeling operator. Both methods are meaningful in the sense that they can help bootstrap the frequency sweeps for full waveform inversion. Joint work with Yunyue Elita Li.}\\
\\ 
    \myaut{Laurent Demanet}\\
    \mail{laurent@math.mit.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Full waveform inversion (FWI) provides accurate subsurface images. In spite of its success, the application of FWI in areas with high-velocity contrast remains a challenging problem.  Quadratic regularization methods are often adopted to stabilize inverse problems. Unfortunately, edges and sharp discontinuities are not adequately preserved by quadratic regularization. During the iterative FWI method, an edge-preserving filter, on the other hand, can gradually incorporate sharpness into seismic images. We use an edge-preserving filter to stabilize FWI.}\\
\\ 
    \myaut{Mauricio Sacchi}\\
    \mail{msacchi@ualberta.ca}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Coherence retrieval is important in optical systems. Mathematically, it is a generalization of phase retrieval problem. In this talk, we will show the error caused by the traditional method for coherence retrieval. Furthermore, a robust trace regularization method is proposed to achieve more robust results.}\\
\\ 
    \myaut{Chenglong Bao}\\
    \mail{clbao@math.tsinghua.edu.cn}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Since 1990, many mathematicians studied the image processing based on partial differential equations and variational methods. Using TV(total variation) and some optimization techniques, there were a lot of improvement in image processing areas. Until deep learning methods coming out, those were the state of art methods in these areas. But once using deep learning techniques, it turns out that in almost every areas in image processing fields, deep learning is the one of the best method. I will explain and compare the some results for Wafer defect detection using the traditional image enhancement methods and deep learning methods. Also I will explain a little bit about the networks of deep learning which we are using for detecting the defect.}\\
\\ 
    \myaut{Myung Joo Kang}\\
    \mail{mkang@snu.ac.kr}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Computational imaging jointly designs optical hardware and computational software to develop methods with new capabilities or simpler hardware than existing methods. Such efficient imaging systems are often sensitive to model mismatch errors (e.g. misalignment, aberrations), requiring careful calibration to remove artifacts. We describe new methods that correct for experimental errors algorithmically, within the inverse problem itself, via joint optimization. This removes the need for time-consuming and delicate calibration routines, resulting in robust and user-friendly setups.}\\
\\ 
    \myaut{Laura Waller}\\
    \mail{waller@berkeley.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Wavelet frame systems are known to be effective in capturing singularities from degraded images. In this talk, we introduce a new edge driven wavelet frame model for image restoration by approximating images as piecewise smooth functions. With an implicit representation of singularity sets, the proposed model inflicts different strength of regularization on smooth region and singularities. Our model is robust to both image approximation and singularity estimation. The implicit formulation also enables to provide a rigorous connection between the discrete model and the continuous variational model.}\\
\\ 
    \myaut{Jae Kyu Choi}\\
    \mail{jaycjk@sjtu.edu.cn}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Blind Image de-convolution is one challenging yet critical problem to many applications, which aims at recovering the clear image from its blurred observation without knowing how it is blurred. In many realistic scenarios, the blurring process is non-stationary in the sense that different image regions are blurred by different kernels, which makes it even more difficult. In this talk, I will present several mathematical models and techniques toward solving non-stationary blind image deblurring problems arising from industrial imaging and digital photography, including spatially-varying blind motion deblurring and defocus map estimation for out-of-focus blurring.}\\
\\ 
    \myaut{Hui Ji}\\
    \mail{matjh@nus.edu.sg}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{While many sophisticated models are developed for visual information processing, very few pay attention to their usability in the presence of data quality degradations. Most successful models are trained and evaluated on high quality visual datasets. On the other hand, the data source often cannot be assured of high quality in practical scenarios. For example, video surveillance systems have to rely on cameras of very limited definitions, due to the prohibitive costs of installing high-definition cameras all around, leading to the practical need to recognize objects reliably from very low resolution images. Other quality factors, such as occlusion, motion blur, missing data and bad weather conditions, are also ubiquitous in the wild. The seminar will present a comprehensive and in-depth review, on the recent advances in the robust sensing, processing and understanding of low-quality visual data, using deep learning methods. I will mainly show how the image/video restoration and the visual recognition could be jointly optimized as one pipeline. Such an end-to-end optimization consistently achieves the superior performance over the traditional multi-stage pipelines. I will also demonstrate how our proposed approach largely improves a number of real-world applications.}\\
\\ 
    \myaut{Zhangyang Atlas Wang}\\
    \mail{atlaswang@tamu.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures tarined via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.}\\
\\ 
    \myaut{Ziming Zhang}\\
    \mail{zzhang@merl.com}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{When taking Cryo-EM images of 3D protein structures, we obtain extremely noisy 2D images. Moreover, it is possible for the same proteins to have different forms under certain circumstances. We want to discuss how to describe such different forms of the same proteins by the unknown covariance matrix representing 3D conformational changes. Since we are most interest in the major deviation in the change, we look for the first principal component via operator norm optimization.}\\
\\ 
    \myaut{Yunho Kim}\\
    \mail{yunhokim@unist.ac.kr}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{In scientific research, sparse approximation scheme has been widely applied to various data processing problems. In this presentation, I will briefly summarize my research work in utilizing sparse approximation and designing novel wavelet frame or low-rank regularization based image restoration, medical imaging and surface reconstruction models. Particularly in my recent work, sparse regularization can be formulated in high dimensional spaces for imaging data defined in or extended to surfaces and manifolds.}\\
\\ 
    \myaut{Jia Li}\\
    \mail{lijia66@mail.sysu.edu.cn}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk, we introduce our novel deep learning approaches called "deep convolutional framelets" for inverse wave scattering problems originated from diffuse optical tomography, ultrasound imaging, wave scattering, etc.  In particular,  inspired by the recent discovery that  a deep convolutional neural network is closely related to the Hankel matrix decomposition,  we provide a unified deep learning approach for addressing  inverse scattering problems that lead to low-rank Hankel matrix and associated deep convolutional neural networks.}\\
\\ 
    \myaut{Jong Chul Ye}\\
    \mail{jong.ye@kaist.ac.kr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this talk, we would like to present how machine learning (including SVR and deep learning) can work for seismic data denoising and inversion from training data sets. We introduce deep learning method to seismic noise attenuation without knowing the noise variance (blind denoising). The training set is obtained by partitioning the data from SEG open data and some denoised field data into small patches. Numerical results are provided with comparisons to traditional methods and state-of-the-art methods, showing deep learning method achieve good performance.}\\
\\ 
    \myaut{Jianwei Ma}\\
    \mail{jma@hit.edu.cn}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{To perform sparse-data CT, the iterative reconstruction commonly uses regularizers in the CS framework. In this paper, inspired by the idea of deep learning, we unfold a state-of-the-art “fields of experts” based iterative reconstruction scheme up to a number of iterations for data-driven training, construct a Learned Experts’ Assessment-based Reconstruction Network (“LEARN”) for sparse-data CT, and demonstrate the feasibility and merits of LEARN network. The experimental results produces a superior performance  to several state-of-the-art methods.}\\
\\ 
    \myaut{Yi Zhang}\\
    \mail{yzhang@scu.edu.cn}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We describe a randomized Newton and randomized quasi-Newton approaches to efficiently solve large linear least-squares problems where the very large data sets present a significant computational burden. In our proposed framework, stochasticity is introduced to overcome these computational limitations, and probability distributions that can exploit structure and/or sparsity are considered. Our results show, that randomized Newton iterates, in contrast to randomized quasi-Newton iterates, may not converge to the desired least-squares solution.}\\
\\ 
    \myaut{Matthias Chung}\\
    \mail{mcchung@vt.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{}\\
\\ 
    \myaut{George Biros}\\
    \mail{biros@ices.utexas.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk, we will be concerned with the question, how well a function, which for instance encodes a classification task, can be approximated by a neural network with sparse connectivity. We will derive a fundamental lower bound on the sparsity of a neural network independent on the learning algorithm, and also demonstrate how networks can be constructed which attain this bound, leading to memory-optimal deep neural networks.}\\
\\ 
    \myaut{Gitta Kutyniok}\\
    \mail{kutyniok@math.tu-berlin.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Principal component pursuit (PCP) is a state-of-the-art approach to background estimation problems. Due to their higher computational cost, PCP algorithms, such as robust principal component analysis (RPCA) and its variants, are not feasible in processing high definition videos. To avoid the curse of dimensionality in those algorithms, several methods have been proposed to solve the background estimation problem incrementally. We build a batch-incremental background estimation model by using a special weighted low-rank approximation of matrices. Through experiments with real and synthetic video sequences, we demonstrate that our model is superior to the existing state-of-the-art background estimation algorithms such as GRASTA, ReProCS, incPCP, and GFL.}\\
\\ 
    \myaut{Aritra Dutta}\\
    \mail{aritra.dutta@kaust.edu.sa}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Generative Adversarial Networks can be used to form sophisticated and accurate models for natural images. However, these models are buried inside a neural network, and can only be accessed in very limited ways.  In this talk, we explore methods to "unpack" image models from neural networks, and use them to form complex image priors and perform difficult image classification tasks. }\\
\\ 
    \myaut{Sohil Shah}\\
    \mail{sohilas@umd.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{This talk presents a new framework for image classification that exploits the relationship between the training of deep Convolution Neural Networks (CNNs) to the problem of optimally controlling a system of nonlinear partial differential equations (PDEs). This new interpretation leads to a variational model for CNNs, which provides new theoretical insight into CNNs and new approaches for designing learning algorithms. We exemplify the myriad benefits of the continuous network in three ways. First, we show how to scale deep CNNs across image resolutions using multigrid methods. Second, we show how to scale the depth of deep CNNS in a shallow-to-deep manner to gradually increase the flexibility of the classifier. Third, we analyze the stability of CNNs and present stable variants that are also reversible (i.e., information can be propagated from input to output layer and vice versa), which in combination allows training arbitrarily deep networks with limited computational resources.}\\
\\ 
    \myaut{Eldad Haber}\\
    \mail{ehaber@eoas.ubc.ca}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We discuss the dynamical systems approach to deep learning, in which training is recast as a control problem and this allows us to formulate necessary optimality conditions using the Pontryagin’s maximum principle (PMP). Modifications of the method of successive approximations is then used to solve the PMP, giving rise to alternative training algorithms for deep learning. Rigorous error estimates are established and applications to training non-traditional networks are explored. }\\
\\ 
    \myaut{Qianxiao Li}\\
    \mail{liqix@ihpc.a-star.edu.sg}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{For many years, discrete optimization models such as conditional random fields (CRFs) have defined the state-of-the-art for classical correspondence problems such as motion and stereo. One of the most important ingredients in those models is the choice of the feature transform that is used to compute the similarity between images patches. For a long time, hand crafted features such as the celebrated scale invariant feature transform (SIFT) defined the state-of-the-art. Triggered by the recent success of convolutional neural networks (CNNs), it is quite natural to learn such a feature transform from data. In this talk, I will show how to efficiently learn such CNN features from data using an end-to-end learning approach. It turns out that our learned models yields state-of-the-art results on a number of established benchmark databases.}\\
\\ 
    \myaut{Thomas Pock}\\
    \mail{pock@icg.tugraz.at}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units, and study the statistical and computational properties in a high-dimensional set-up, where the number of hidden units grow unbounded. While these networks are provably adaptive to low-dimensional structures, they require an efficient solver for the non-convex subproblem of addition of a new unit.}\\
\\ 
    \myaut{Francis Bach}\\
    \mail{francis.bach@ens.fr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In the context of regularisation of ill-posed problems, we propose to extend a specific parameter learning approach to also include misfit training data. We demonstrate that this allows to learn more adaptive regularisations in contrast to the same approach without additional misfit data. The incorporation of misfit data is modelled as a minimisation of quotients of parametric regularisation functions. We discuss how to solve this non-convex optimisation problem numerically and conclude with numerical results.  This is joint work with Guy Gilboa (Technion, Haifa), Joana Sarah Grah (University of Cambridge) \& Carola-Bibiane Schönlieb (University of Cambridge).}\\
\\ 
    \myaut{Martin Benning}\\
    \mail{mb941@cam.ac.uk}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Adversarial neural networks solve many important problems in image science, and can be used to build sophisticated image models and priors.  However, these models are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to "collapse", and enables faster training with larger learning rates.}\\
\\ 
    \myaut{Tom Goldstein}\\
    \mail{tomg@cs.umd.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk, we present our latest work on learning reconstructions of low-dose computed tomography data.
We focus on two methods to decrease the radiation dose: x-ray tube current reduction, reducing the signal-to-noise ratio, and x-ray beam interruption, which undersamples data and results in images with aliasing artifacts. 
Our reconstruction results using trainable variational networks enable higher radiation dose reductions and/or increase the image quality for a given dose.}\\
\\ 
    \myaut{Erich Kobler}\\
    \mail{erich.kobler@icg.tugraz.at}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{With RAISR (Rapid and Accurate Image Super-Resolution) we have introduced a technique that incorporates not-so-deep machine learning in order to produce high-quality versions of low-resolution images. In this framework, with sufficient training data (corresponding pairs of low and high resolution images) we can learn sets of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is low complexity. RAISR produces results that are comparable to or better than the currently available super-resolution methods, and does so roughly 10 to 100 times faster, allowing it to be run on a typical mobile device in real-time. Furthermore, our approach is general enough that it can be deployed successfully to a much wider set of enhancement operators besides super-resolution. }\\
\\ 
    \myaut{Peyman Milanfar}\\
    \mail{peyman.milanfar@gmail.com}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit instead of explicit gradient steps to update the network parameters during neural network training. ProxProp is developed from a general point of view on the backpropagation algorithm. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy. We conclude by analyzing theoretical properties of ProxProp and demonstrate promising numerical results.}\\
\\ 
    \myaut{Thomas Frerix}\\
    \mail{thomas.frerix@tum.de}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{The past few years have seen a dramatic increase in the performance of recognition systems thanks to the introduction of deep networks for representation learning. However, the mathematical reasons for this success remain elusive. A key issue is that the neural network training problem is non-convex, hence optimization algorithms may not return a global minima. Building on ideas from convex relaxations of matrix factorizations, this work proposes a general framework which allows for the analysis of a wide range of non-convex factorization problems – including matrix factorization, tensor factorization, and deep neural network training. The talk will describe sufficient conditions under which a local minimum of the non-convex optimization problem is a global minimum and show that if the size of the factorized variables is large enough then from any initialization it is possible to find a global minimizer using a local descent algorithm. Joint work with Ben Haeffele.}\\
\\ 
    \myaut{Rene Vidal}\\
    \mail{rvidal@jhu.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We study how to model the texture as a stationary and ergodic random process. The texture is assumed to be homogenous in two-dimensional space, i.e. stationary under translation, and is ergodic in sense that the statistics do not differ considerably from one single realisation to another. These statistical properties fit well the classical microcanonical modeling framework, resulting in a maximum entropy process satisfying a pre-defined set of statistics.
\
The main challenge is to define this set of statistics so as to capture the non-Gaussianarity of the underlying random process, in particular when the texture has long-range dependency and complex geometry such as vorticity solutions two-dimensional Navier-Stokes equation. One common way to evaluate the model is to synthesis texture of similar visual quality and variability.
\
We use the scattering transform to capture the geometric correlations in the space beyond second order statistics. 
The basic idea is to model progressively the Gaussian and non-Gaussian part of the random process through a cascade of convolutional and non-linear transforms. When the random process is a Gaussian process, we use the $l\_2$ moments of the 1st order scattering coefficients to approximate its covariance structure. We evaluate the Gaussian model by standard metrics in power spectrum estimation. 
\
The use of the Gabor wavelets in 1st order scattering also extracts the non-Gaussian statistics. The non-linear modulus operator is then applied to introduce the correlations between across scales and angles. We then build a second order model with PCA to capture these non-Gaussian correlations by the 2nd order scattering coefficients. We will present results on the synthesis texture of Turbulence and Brodatz texture dataset. }\\
\\ 
    \myaut{Sixin Zhang}\\
    \mail{sixin.zhang@ens.fr}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{}\\
\\ 
    \myaut{Christoph Brune}\\
    \mail{c.brune@utwente.nl}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In computed tomography (CT), one of the key issues is the limited angle problem. Traditional imaging methodologies are not capable of reconstructing the complete image satisfactorily. In this talk, we will present a deep learning approach to this problem using shearlets as a sparsifying transform.}\\
\\ 
    \myaut{Gitta Kutyniok}\\
    \mail{kutyniok@math.tu-berlin.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We discuss image generation using WassersteinGANs in an adversarial-free primal form. This approach has many advantages, in particular it is mathematically well-posed and results in an optimization problem that is easier and more robust than the standard adversarial paradigm. Unfortunately, it also has a disadvantage: it does not work. The learned generators reproduce the training set (in a noise and blurry way), but do not generate new natural images. In my talk, I will discuss the reason for this negative result and what it teaches us about GANs in general.}\\
\\ 
    \myaut{Christoph Lampert}\\
    \mail{chl@ist.ac.at}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We show that stochastic gradient descent (SGD) performs variational inference, it minimizes an average potential over the posterior distribution on weights with an entropic regularization term. For deep networks, this potential is different from the original loss used to compute gradients due to highly non-isotropic mini-batch gradient noise. Most likely trajectories of SGD in this case are not Brownian motion near critical points, they are closed loops in the weight space. Joint work with Stefano Soatto.}\\
\\ 
    \myaut{Pratik Chaudhari}\\
    \mail{pratikac@ucla.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{A bilevel texture model is proposed, based on a local transform of a Gaussian random field. The core of this method relies on the optimal transport of a continuous Gaussian distribution towards the discrete exemplar patch distribution. The synthesis then simply consists in a fast post-processing of a Gaussian texture sample, boiling down to an improved nearest-neighbor patch matching, while offering theoretical guarantees on statistical compliancy. }\\
\\ 
    \myaut{Arthur Leclaire}\\
    \mail{arthur.leclaire@cmla.ens-cachan.fr}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We introduce optimal transport-type distances for manifold-
valued images. To do so we lift the initial data to measures on the product space of image domain and signal space, where they are compared using a transport cost that combines spatial distance and signal discrepancy. Applying recent ‘unbalanced’ optimal transport models leads to more natural results. We illustrate the benefit of the lifting with examples for interpolation of color images and classification of handwritten digits.
}\\
\\ 
    \myaut{Friederike Laus}\\
    \mail{friederike.laus@mathematik.uni-kl.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The geometry of optimal transport provides a useful way to compare and interpolate histograms. Recent work on entropic regularization makes optimal transport computationally affordable and easy to implement. This talk presents Wasserstein Barycentric Projection and Wasserstein Dictionary Learning as an optimization over parameters within the entropic optimal transport framework. I will show numerous applications in computer graphics, for processing colors, geometry, images and material reflectances.}\\
\\ 
    \myaut{Nicolas Bonneel}\\
    \mail{nicolas.bonneel@liris.cnrs.fr}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{First, a new patch-based method for exemplar-based image style transfer is presented. It is based on an adaptive partition that captures the style of the example image and preserves the structure of the source image.
Then, an extension to video is also proposed ensuring spatially and temporally consistent stylized videos. Results show that out method is visually plausible while being very competitive in memory and computation time.}\\
\\ 
    \myaut{Neus Sabater}\\
    \mail{neus.sabater@technicolor.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Rapidly developing technology of photon-counting or energy-discriminating detectors has provided an additional spectral dimension to conventional X-ray grayscale imaging.The energy-binned data, however, suffer from low signal-to-noise ratio, acquisition artifacts, and frequently angular undersampled conditions. Since energy-channels are mutually correlated it can be advantageous to incorporate additional knowledge into the reconstruction algorithm. We propose a novel iterative method which jointly reconstructs all energy channels while imposing a strong structural correlation between them.}\\
\\ 
    \myaut{Daniil Kazantsev}\\
    \mail{daniil.kazantsev@manchester.ac.uk}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{By considering the gradient of a multichannel image as a 3D tensor with dimensions corresponding to the image domain, spatial derivatives and channels, we introduce collaborative sparsity enforcing norms to address the ill-posedness of color imaging problems. We obtain a regularization framework for color images that uses different channel couplings and enables joint directions of smoothing. We analyze which of the arising models are best suited for color image denoising and other inverse problems.}\\
\\ 
    \myaut{Joan Duran}\\
    \mail{joan.duran@uib.es}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Spectral CT has the potential to vastly increase the quality and information content from CT scans, while using the same dose as a conventional scan.
\
To reach this full potential, all stages of the tomography pipeline, ranging from data preprocessing and reconstruction to segmentation and analysis need to be revisited and redesigned.
\
In this talk I will highlight the various aspects of designing a spectral tomography pipeline, illustrated by our concrete experiences in the lab.}\\
\\ 
    \myaut{Joost Batenburg}\\
    \mail{joost.batenburg@cwi.nl}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Recent years witnessed a proliferation of hard x-ray imaging instruments at the synchrotron and laboratory-based facilities that can provide resolutions down to few tens of nanometers. However, tomography at these fine scales is particularly challenging, because of the reduced photon efficiency, higher radiation doses, and inaccuracies of the stages, which leads to noisy and sparse datasets. In this talk, I will first give recent developments for scanning-based x-ray imaging and microscopy applications, and then describe how existing and new algorithmic approaches can be adopted to enable faster and reliable information extraction from complex multi-dimensional measurement data. }\\
\\ 
    \myaut{Doga Gursoy}\\
    \mail{dgursoy@aps.anl.gov}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{A novel compressive imaging model is proposed that multiplexes segments of the field of view onto an infrared focal plane array.  Similar to compound imaging, our model is based on combining pixels from a surface comprising of different parts of the FOV.  We formalize this superposition of pixels in a global multiplexing process reducing the number of detectors required of the FPA.  We present an analysis of the Signal-to-Noise Ratio (SNR) for the full rank and compressive collection paradigms for a target detection and tracking scenario. }\\
\\ 
    \myaut{Robert Muise}\\
    \mail{robert.r.muise@lmco.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Recent work in computational and compressive imaging has shown that a relatively coarse array of pixels in conjunction with a spatial light modulator can be used to reconstruct a high-resolution image. These transform-domain measurements can also be used directly in machine learning applications for classification. We report on recent progress using relatively few 64x64 SWIR FPA frames to either reconstruct high-resolution images, or to replace the first two layers of a typical CNN and monitor a scene of interest.}\\
\\ 
    \myaut{Matthew Herman}\\
    \mail{matthew.herman@inviewcorp.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We consider the question of estimating a solution to a system of equations that involve convex nonlinearities, a problem that is common in machine learning and signal processing. Because of these nonlinearities, conventional estimators based on empirical risk minimization generally involve solving a non-convex optimization program. We propose a method (called "anchored regression”) that is based on convex programming and amounts to maximizing a linear functional (perhaps augmented by a regularizer) over a convex set. 
\
The proposed convex program is formulated in the natural space of the problem, and avoids the introduction of auxiliary variables, making it computationally favorable. Working in the native space also provides us with the flexibility to incorporate structural priors (e.g., sparsity) on the solution.
\
For our analysis, we model the equations as being drawn from a fixed set according to a probability law.  Our main results provide guarantees on the accuracy of the estimator in terms of the number of equations weare solving, the amount of noise present, a measure of statistical complexity of the random equations, and thegeometry of the regularizer at the true solution. We also provide recipes for constructing the anchor vector (that determines the linear functional to maximize) directly from the observed data.
\
We will discuss applications of this technique to nonlinear problems including phase retrieval, blind deconvolution, and inverting the action of a neural network.
\
This is joint work with Sohail Bahmani.
}\\
\\ 
    \myaut{Justin Romberg}\\
    \mail{jrom@ece.gatech.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{To achieve a wide field of view without some of the disadvantages of conventional wide field of view systems, multiplexed imaging systems superimpose multiple images onto a common focal plane. Recovery of a wide field of view conventional image from these multiplexed measurements typically requires multiple frames. This implicitly assumes a static scene, effectively imposing a sampling rate requirement, which in turn makes real time performance challenging to achieve. This talk will describe work on reducing scene sampling requirements, relaxing the static scene assumption, and improving the computational complexity of image formation algorithms. Results will be shown in simulation and using an infrared wide field of view multiplexed sensor developed at MIT Lincoln Laboratory based on a novel division of aperture sensor architecture. Finally, we will highlight recent developments of optically multiplexed sensors at our laboratory and relate them to the problems we discussed.}\\
\\ 
    \myaut{Yaron Rachlin}\\
    \mail{yaron.rachlin@ll.mit.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{This talk will describe new microscopes that use computational imaging – the joint design of optical systems and inverse algorithms - to enable 3D imaging from a single-shot with a lensless camera consisting of only a scattering element (diffuser) placed in front of a 2D image sensor. Our reconstruction algorithms are based on large-scale nonlinear non-convex optimization with sparsity-based regularizers similar to compressed sensing. }\\
\\ 
    \myaut{Laura Waller}\\
    \mail{waller@berkeley.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{}\\
\\ 
    \myaut{Richard Baraniuk}\\
    \mail{richb@rice.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Using an information-theoretic approach we quantify the fundamental limits of imaging and sensing around the corner/obstacles using passive measurements. We show that using lightfield measurements one can estimate object position and achieve low-resolution imaging using a high-fidelity forward model.}\\
\\ 
    \myaut{Amit Ashok}\\
    \mail{ashoka@email.arizona.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We present a framework to compute lower bounds for parameter estimation from noisy plenoptic observations. Our particular focus is on indirect imaging problems, where the observations do not contain line-of-sight information about the parameter(s) of interest. Using computer graphics rendering software to synthesize the (often complicated) dependence among parameter(s) of interest and observations, we numerically evaluate Barankin bounds for these tasks.  We demonstrate our results on some canonical example scenes.}\\
\\ 
    \myaut{Abhinav V. Sambasivan}\\
    \mail{samba014@umn.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{}\\
\\ 
    \myaut{Aristide Dogariu}\\
    \mail{adogariu@creol.ucf.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Active optical sensing is used for time of flight measurements and imaging in degraded vision condition. However, classical optical imaging relays on lines-of-sight sampling and ranging is bandwidth limited. Recently, computational sensing approaches have brought new sensing capabilities and enhanced the spatial and temporal resolution. Now, active imaging devices are able to image and track objects outside the direct field of view and to measure ranges with super-resolution. }\\
\\ 
    \myaut{Martin Laurenzis}\\
    \mail{martin.laurenzis@isl.eu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Angular memory effect imaging is a technique for imaging through an intervening scattering medium. The general method has a number of highly-specific constraints which limit applicability of the technique. In this talk I will discuss how coding and dictionary methods from computational and compressive sensing can overcome some of these limitations, particularly with respect to temporal and spectral bandwidths of the source and the resulting contrast of the speckle patterns recorded at the detector. }\\
\\ 
    \myaut{Michael Gehm}\\
    \mail{michael.gehm@duke.edu}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Target detection on a high resolution midwave infrared focal plane array is cost prohibitive in some applications.  But, due to the compressibility of infrared image patches, the high resolution requirement could be reduced with target detection capability preserved. As the most probable coefficient indices of the support set of the infrared image patches could be learnt from the training data, we develop STLS for MWIR image reconstruction. Using the same measurement matrix as in STLS, we construct CQCF for compressed infrared target detection. Numerical simulations show that the recognition performance of our algorithm matches that of the standard full reconstruction methods but at a fraction of the execution time.}\\
\\ 
    \myaut{Brian Millikan}\\
    \mail{brian.millikan@lmco.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We present two representer theorems that provide the parametric form of the solution(s) of generic linear inverse problems with Tikhonov (p=2) vs. total-variation (p=1) regularization. Remarkably, the solutions in both cases are generalized splines that are tied to the underlying regularization operator L. For p=2, the knots are fixed with basis functions that are smoothed versions of the measurement operator. In the total variation scenario, the solutions are nonuniform L-splines with adaptive (and fewer) knots.}\\
\\ 
    \myaut{Michael Unser}\\
    \mail{michael.unser@epfl.ch}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Tomographic imaging from limited projections is an ill-posed problem and reconstruction algorithms rely on regularization, often sparsity-based, to improve the quality of imaging. We present a spline framework for consistent discretization in tomographic reconstruction and demonstrate its advantages for sparse approximation. Our experiments provide comparisons with commonly-used techniques such as total variation based tomographic reconstruction.}\\
\\ 
    \myaut{Alireza Entezari  }\\
    \mail{entezari@ufl.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{}\\
\\ 
    \myaut{Mathews Jacob}\\
    \mail{mathews-jacob@uiowa.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Discretization—representing a continuous-time function or operation with a discrete-time one—is unavoidable in solving inverse problems. In X-ray computed tomography (CT) reconstruction, the classical algorithm handles discretization "at the end". Modern approaches discretize "in the middle" or "at the beginning". In this talk, I will show how the latter provides algorithms that are mathematically rigorous and implementable. I will also discuss the choice of the basis function among pixels, B-splines, box splines, Kaiser-Bessel windows, and sincs.}\\
\\ 
    \myaut{Michael McCann}\\
    \mail{michael.mccann@epfl.ch}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We propose an adaptive algorithm for sparse image approximation, termed “adaptive thinning” (AT). The algorithm AT works with recursive removals of pixels from the target image, where the image is approximated by linear splines over anisotropic Delaunay triangulations. We discuss both computational and theoretical aspects of AT. The good performance of AT is finally supported 
by numerical examples and comparisons.}\\
\\ 
    \myaut{Armin Iske}\\
    \mail{armin.iske@uni-hamburg.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Hermite splines are commonly used for interpolating data when samples of the derivative are available, in a scheme
called Hermite interpolation. Assuming a suitable statistical model, we demonstrate that this method is optimal for
reconstructing random signals in Papoulis’ generalized sampling framework. More precisely, we show the equivalence between cubic Hermite interpolation and the linear minimum mean-square error (LMMSE) estimation of a second-order Lévy process. }\\
\\ 
    \myaut{Virginie Uhlmann}\\
    \mail{virginie.uhlmann@epfl.ch}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We present a robust approach to perform 3D nonrigid image registration suitable for large deformation and develop a software named DTHB3D\_Reg. The optimum spatial transformation, defined using truncated hierarchical B-splines, is obtained through the minimization of an energy functional. An adaptive strategy carries out refinement only in the regions with large deformation. The proposed method is demonstrated on medical images to show robustness on topology change as compared to other image registration methods.}\\
\\ 
    \myaut{Aishwarya Pawar}\\
    \mail{arpawar@andrew.cmu.edu}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{We present a study concerning the construction, the properties and the applications in image processing of a family of nonstationary biorthogonal wavelet filterbanks. Such a family is generated by a class of functions satisfying level-dependent refinement equations and includes cardinal polynomial B-splines. Nonstationarity offers a greater flexibility and a better adaptation to the local image content, allowing for a more focused scale-space analysis and thus providing better results when compared to classical biorthogonal B-spline filters.}\\
\\ 
    \myaut{Vittoria Bruni}\\
    \mail{vittoria.bruni@sbai.uniroma1.it}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this talk we present a new family of active contours by exploiting subdivision schemes. Depending on the choice of the mask, such models have the ability to reproduce trigonometric or polynomial curves. They can also be designed to be interpolating, a property that is useful in user-interactive applications. Such active contours are robust in the presence of noise and to the initialization. We illustrate their use for the segmentation of bioimages.
}\\
\\ 
    \myaut{Anaïs Badoual}\\
    \mail{anais.badoual@epfl.ch}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{The aim of this talk is to discuss the construction of minimally supported basis functions for Hermite interpolation on a three directional mesh of the plane. Our model relies on three directional Box-splines and gets advantage from the deep relationship between Hermite and Bézier representation of piecewise bivariate polynomials. Starting by the simpler but analogous univariate case, we will show how the use of Greens' function allows us to unreveil all theoretical properties of the new bivariate Hermite basis functions.
\
The proposed model meets practical requirements such as invariance to affine transformations and good approximation properties.One of its great advantages is its non tensor-product structure which avoids the use of mixed derivatives and makes it suitable to Hermite-like representation of images in terms of samples with local tangents.}\\
\\ 
    \myaut{Costanza  Conti}\\
    \mail{costanza.conti@unifi.it}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{In nonrigid image registration, B-splines are used extensively for interpolation and transformation. We propose efficient multi-dimensional algorithms for the B-spline interpolation and transformation functions and their derivatives. The algorithms are based on recursive formulations and implemented using template metaprogramming. Compared to reference implementations, we obtain an acceleration factor of 4 for interpolation and a factor of 18 for transformation. When used within a registration algorithm, total computation time reduces by a factor 1.5 to 3.5.}\\
\\ 
    \myaut{Stefan Klein}\\
    \mail{s.klein@erasmusmc.nl}\\\\
    \noindent\textbf{MS Part 3}\\
\\  
    \textit{Magnetic resonance fingerprinting methods quantify multiple MR parameters by matching the measured signal to a precomputed 5-dimensional dictionary in which each parameter is a dimension. Generating high-precision maps requires dense grids in each dimension, which is prohibitively expensive in memory and computation time. We propose B-spline interpolation of the dictionary to reduce the dictionary size and to enable efficient nonlinear least-squares fitting by gradient-based optimization methods. The method is shown to substantially reduce fitting error.}\\
\\ 
    \myaut{Willem van Valenberg}\\
    \mail{w.vanvalenberg@tudelft.nl}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Reconstruction in inverse problems is often one step in a procedure where the reconstructions is used for decision making associated to a task. The talk will use statistical decision theory for extending recent iterative learned methods to include tasks that can be formulated as a supervised learning task, e.g., segmentation, comparison, classification, registration, or caption generation. We will outline this framework and show examples of task based reconstructions in the context of tomographic imaging.}\\
\\ 
    \myaut{Ozan  Öktem}\\
    \mail{ozan@kth.se}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We improve image reconstruction from sparse data in photoacoustic tomography using a deep convolutional network. The weights of the convolutional network are adjusted prior to the actual image reconstruction through training with pairs of reconstructions including artifacts and the corresponding artifact-free images. We demonstrate with simulated and experimental data that the proposed approach provides reconstructed images with a quality comparable to state of the art approaches and reduced numerical costs.}\\
\\ 
    \myaut{Johannes Schwab}\\
    \mail{johannes.schwab@uibk.ac.at}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We give an overview of algorithms for compressed sensing that are based on neural networks. We experimentally show (using simulated data) that methods using convolutional neural networks are faster than traditional algorithms based on convex optimization, while giving similar results. We then investigate the possible real world usage of this methods for the compression of data acquired by a Time-of-Flight camera.}\\
\\ 
    \myaut{Stephan Antholzer}\\
    \mail{stephan.antholzer@uibk.ac.at}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Recently, deep learning approaches with various network architectures have achieved significant performance improvement over existing iterative reconstruction methods in various inverse problems. However,   it is still unclear  why these deep learning architectures work.  Here we show that  the long-searched-for missing link is the deep convolutional framelets expansion for representing a signal by convolving local and non-local bases using multi-level decomposition. Using numerical experiments with various inverse problems,  we confirm the validity of our discovery.}\\
\\ 
    \myaut{Jong Chul Ye}\\
    \mail{jong.ye@kaist.ac.kr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{We consider the problem, that has various applications in imaging, of finding a pair of points, in two convex sets, such that the distance between the points is as small as possible. Assuming that the convex sets are polyhedrons, and that the angle between any pair of faces of these polyhedrons is lower bounded by a positive constant, we will describe the local as well as the global convergence rate of a widely-used algorithm that solves this problem: the Douglas-Rachford method.}\\
\\ 
    \myaut{Irene Waldspurger }\\
    \mail{waldspurger@ceremade.dauphine.fr}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Using deep learning, we demonstrate significant advances in different modes of microscopic imaging, including bright-field, holographic and mobile-phone based microscopy tools, increasing their imaging throughput, resolution and depth-of-field, while also eliminating or correcting for undesired spatial and/or spectral artifacts. This deep learning based framework can be broadly applied for solving inverse problems in computational microscopic imaging, and especially benefit imaging modalities where an accurate modeling of the image formation process is challenging.}\\
\\ 
    \myaut{Yair Rivenson}\\
    \mail{rivenson@gmail.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We propose and compare two different machine learning approaches for cardiac MR image reconstruction. The first class of approaches uses dictionary learning techniques for compressed sensing MR image reconstruction. The second class of approaches uses convolutional neural networks for MR image reconstruction. We evaluate both approaches in terms of reconstruction quality and computational speed.}\\
\\ 
    \myaut{Daniel  Rueckert}\\
    \mail{d.rueckert@imperial.ac.uk}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk, we will review recent progresses on deep learning for photoacoustic tomography.}\\
\\ 
    \myaut{Linh Nguyen}\\
    \mail{lnguyen@uidaho.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In photoacoustic imaging, as in many high resolution modalities, the major bottle neck is the acquisition time of finely spatially sampled data. In particular for imaging of dynamical processes, this results in incomplete data. In this talk we are going to discuss different aspects of variational methods for photoacoustic imaging using spatio-temporal regularization via optical flow constraint which allow us maintaining good quality of the reconstruction from severely subsampled/compressed dynamic data.}\\
\\ 
    \myaut{Marta Betcke}\\
    \mail{m.betcke@ucl.ac.uk}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We endow the inhomogeneous wave equation with time-dependent parameters and consider the task of reconstructing these parameters from the wave field. This dynamic inverse problem is more involved compared to static parameters. We give existence and uniqueness results for the equation and compute the Fréchet derivative of the solution operator, for which we also show ill-posedness. These results motivate the numerical reconstruction using regularized Newton-like methods.}\\
\\ 
    \myaut{Thies Gerken}\\
    \mail{tgerken@math.uni-bremen.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Motion compensation represents an important time-dependent problem in tomography. Most modalities record the data sequentially, e.g. in computerized tomography, measurements are taken while the X-ray source rotates around the specimen. Temporal changes of the object therefore lead to undersampled and/or inconsistent measurements. Consequently, suitable models and algorithms have to be developed in order to provide artefact free images. In this talk, we present recent advances in this field and their application to different imaging modalities.}\\
\\ 
    \myaut{Bernadette Hahn}\\
    \mail{bernadette.hahn@mathematik.uni-wuerzburg.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In many clinical applications the tracking of fast dynamics in 4D MR data is of major interest. Due to time restrictions in the MR measurement process in this context it is reasonable to make use of undersampled data. In this talk we discuss variational methods for spatial temporal reconstruction from such MR data, which decompose the image sequence into different characteristic components such as background, motion and inhomogeneities. }\\
\\ 
    \myaut{Meike Kinzel}\\
    \mail{meike.kinzel@uni-muenster.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{In this work we consider the inverse problem of reconstructing the optical properties of a dielectric medium from measurements from the multi-modal Photoacoustic and Optical Coherence Tomography (PAT/OCT) setup.  We reformulate the inverse problem as a Fredholm type integral equation for the Grüneisen parameter (space dependent) to be solved with a Galerkin type method. 
This is a joint work with P. Elbau and O. Scherzer.  }\\
\\ 
    \myaut{Leonidas Mindrinos}\\
    \mail{leonidas.mindrinos@univie.ac.at}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Time-dependent problems are often modeled via parabolic differential equations. Possibly, the most prominent example for such an equation is the classical linear heat equation that describes the distribution of heat (or temperature) in a given region over time. We discuss extensions of this classical equation to nonlinear PDEs in 1D. Furthermore, we will address the associated inverse heat transfer problems and their relevance in industrial applications. Finally, numerical studies will be presented.}\\
\\ 
    \myaut{Dimitri Rothermel}\\
    \mail{rothermel@math.uni-sb.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Image registration is the task of establishing correspondences between two or more images. To perform the registration of multiple images simultaneously, we briefly describe an approach based on singular value decomposition of the matrix which consists of the images‘ gradients. We use this method to align dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) data sets of human kidneys (data courtesy of Jarle Rørvik, Haukeland University of Bergen) and analyze the corresponding data by pharmacokinetical modelling.}\\
\\ 
    \myaut{Benjamin Wacker}\\
    \mail{wacker@mic.uni-luebeck.de}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{A large number of inverse problems in applications ranging from engineering via economics to systems biology can be formulated as a state space system with a finite or infinite dimensional parameter that is supposed to be identified from additional continuous or discrete observations. In this talk we will compare reduced and all-at-once versions of Landweber-Kaczmarz iteration for a reformulation of the problem as a system resulting from splitting the time line into
subintervals.}\\
\\ 
    \myaut{Tram Thi Ngoc Nguyen}\\
    \mail{tram.nguyen@aau.at}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The residual method consists in minimizing a regularization functional over a feasible set defined by fidelity constraints. These fidelity constraints can account for errors in the forward operator, however, in the classical setting in normed spaces, this results in a non-convex optimization problem. In partially ordered spaces fidelity constraints can be expressed in a way that yields convex feasible sets. We will present the theory and applications in deblurring with uncertainty in the blurring kernel.}\\
\\ 
    \myaut{Yury Korolev}\\
    \mail{korolev.msu@gmail.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{The conventional way of formulating inverse problems, such as parameter identification in PDEs, is via some forward operator, which is the composition of the observation operator with the parameter-to-state-map for the underlying PDE model. Recently, all-at-once formulations have been considered as an alternative to this reduced formulation, avoiding the numerical evaluation of a parameter-to-state-map. Here the model and the observation are considered simultaneously as one large system with the state and the parameter as unknowns.}\\
\\ 
    \myaut{Barbara Kaltenbacher}\\
    \mail{barbara.kaltenbacher@aau.at}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Models used in inverse problems are never complete and even more importantly such models, perhaps optimal in given function spaces, hardly cover realistic inputs stemming from only a subset of the space of the unknowns, which however is unknown. Hence, data driven corrections to analytical models can compensate for both shortcomings.
\
We analyze mathematically neural networks for solving almost trivial inverse problems and discuss data driven updates to the analytical model of magnetic particle imaging.}\\
\\ 
    \myaut{Peter Maass}\\
    \mail{pmaass@math.uni-bremen.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{In this talk we derive and analyze a variational model for the joint estimation of motion and reconstruction of image sequences, which is based on a time-continuous Eulerian motion model that can be set up in terms of the continuity equation or the brightness constancy equation. We rigorously prove the existence of a minimizer in a suitable function space setting, discuss the numerical solution of the model based on primal-dual algorithms and investigate several examples.}\\
\\ 
    \myaut{Carola-Bibiane Schönlieb}\\
    \mail{cbs31@cam.ac.uk}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{This talk presents a general Bayesian computation framework for performing inference in inverse problems with forward models that are partially unknown. A main novelty of the framework is that it uses MCMC-driven stochastic optimisation methods that tightly integrate modern high-dimensional Monte Carlo sampling and convex optimisation approaches. The proposed methodology is illustrated on a range of challenging imaging problems and compared to other techniques from the state of the art.}\\
\\ 
    \myaut{Marcelo Pereyra}\\
    \mail{m.pereyra@hw.ac.uk}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{The estimation and visualization of active parts of the brain from EEG recordings is referred to as the EEG source imaging. The source estimation is highly sensitive to modeling uncertainties of the computational model, especially the electrical properties of the head tissues.  We show that the exact knowledge of the skull conductivity is not always necessary, since it can be taken into account statistically in the inversion by using the Bayesian approximation error approach.}\\
\\ 
    \myaut{Alexandra  Koulouri}\\
    \mail{a.koulouri84@gmail.com}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Dynamic X-ray tomography has received particular attention in the last years, since it can reduce the amount of needed projections and by that minimize radiation exposure. In a dynamic setting the data consists of time-dependent projection measurements of a non-stationary object, e.g. organs in medical computerized tomography (CT), which can lead to significant artifacts in a stationary reconstruction. Here, we discuss a reconstruction method related to data obtained from randomized angles.}\\
\\ 
    \myaut{Tapio Helin}\\
    \mail{tapio.helin@helsinki.fi}\\\\
    \noindent\textbf{MS Part 2}\\
\\  
    \textit{Many inverse problems can be cast as a PDE-constrained optimization problem with non-linear equality constraints. By enforcing these constraints, however, we are tacitly assuming that the underlying PDE model is an accurate description of the underlying process. To account for imperfect models, I propose a relaxed formulation that treats the constraints through an additive penalty. I discuss ways to design efficient algorithms through the use of variable projection and present some numerical examples.}\\
\\ 
    \myaut{Tristan van Leeuwen}\\
    \mail{t.vanleeuwen@uu.nl}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{This talk will first present the main topics covered in this minisymposium. Then an overview of current active research in nonlinear spectral processing will be given, with focus on activities in my lab. This includes spectral representations of non-convex functionals, 1-Laplacian theory and applications and new numerical methods to solve nonlinear eigenvalue problems.  }\\
\\ 
    \myaut{Gilboa Guy}\\
    \mail{guy.gilboa@ee.technion.ac.il}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{I will give an overview of our work on nonlinar eigenproblems ranging from exact relaxations of combinatorial problems as nonlinear eigenproblems to our recent work on Perron-Frobenius theory of multi-homogeneous mappings related to spectral problems of tensors and hypergraphs. This also leads to a nonlinear spectral
method with which one can train a certain neural network globally optimal with a linear convergence rate.}\\
\\ 
    \myaut{Hein Matthias}\\
    \mail{hein@cs.uni-saarland.de}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We present a method to isolate and differentiate objects of different contrasts, sizes and structures in complex, multi-scaled sensor imagery. The analysis is based on time and spatial information of spectral TV bands, referred to as spectral time signatures. We use dimensionality reduction to extract highly contrasted objects in thermal and medical images for image fusion; as well as groups of objects of similar features in images of repetitive structures for image manipulation.  }\\
\\ 
    \myaut{Hait Ester}\\
    \mail{etyhait@campus.technion.ac.il}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Reliable and automated segmentation of objects of different scales is a key problem in the field of medical imaging. Recently, new theory and algorithms for nonlinear eigenvalue problems via spectral decompositions have been developed and shown to result in promising segmentation results. In this talk, we compare different data-terms for TV-based segmentation and evaluate how informative the resulting spectral response function is. The analysis is supported by segmentation results of simulated and experimental cell data.}\\
\\ 
    \myaut{Zeune Leonie}\\
    \mail{l.l.zeune@utwente.nl}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Motivated by applications in machine learning we consider the notion of total variation defined on graphs over  data clouds. Graphs are used to leverage the geometry of a ground-truth distribution, as well as to incorporate "must link" and "can not link" constraints on the data. We study the variational limit of the graph total variation as the number of data points goes to infinity and the parameters used to construct the graphs are scaled appropriately.}\\
\\ 
    \myaut{Garcia Trillos Nicolas}\\
    \mail{nicolas\_garcia\_trillos@brown.edu}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We introduce a set-pair Lovàsz extension which is used to construct equivalent continuous optimization problems for graph cut including the maxcut, Cheeger cut, dual Cheeger cut, etc. With the help of the explicit objective functions of resulting continuous optimizations, we develop the spectral theory of graph 1-Laplacian for studying both Cheeger cut and maxcut. Elementary facts, rich structures and striking advantages of 1-Laplacian will be fully revealed.}\\
\\ 
    \myaut{Zhang Dong}\\
    \mail{13699289001@163.com}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{We present a two-step method to reduce bias in variational methods. After solving the standard variational problem, the idea is to perform a consecutive debiasing step minimizing the data fidelity on an appropriate model subspace. It is defined by Bregman distances using the subgradient appearing in the optimality condition of the variational method. This leads to a decomposition of the overall bias into two parts, model and method bias, of which we tackle the latter.}\\
\\ 
    \myaut{Camille Sutour}\\
    \mail{camille.sutour@parisdescartes.fr}\\\\
    \noindent\textbf{MS Part 1}\\
\\  
    \textit{Nonlinear eigenfunctions, induced by subgradients of one-homogeneous
functionals (such as the 1-Laplacian), have shown to be instrumental in
segmentation, clustering and image decomposition.  We present a class
of flows for finding such eigenfunctions, generalizing a method recently
suggested by Nossek-Gilboa.  We analyze the flows on grids and graphs
in the time-continuous and time-discrete settings. 
Several examples are provided showing how such flows can be
used on images and graphs.}\\
\\ 
    \myaut{Aujol Jean-Francois}\\
    \mail{jean-francois.aujol@math.u-bordeaux.fr}\\\\
\end{multicols}

